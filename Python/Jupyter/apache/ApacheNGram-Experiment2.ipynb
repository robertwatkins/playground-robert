{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for Malicious Users Through Apache Log Analysis - Experiment 2\n",
    "\n",
    "### Introduction\n",
    "The inspiration for this project was to determine if/how malicous users could be identified by using machine learning tools to analyze Apache Web Server Logs. \n",
    "\n",
    "For this exercise, I use the freely-available Apache log samples available from NASA for one of their web servers from 1995. I'll start by identifying the kinds of data in the log file then isolate the activity on the site by user and perform [n-gram](https://en.wikipedia.org/wiki/N-gram) analysis to categorize new users as being 'typical' or 'a-typical'. The presumption is that 'a-typical' users would need to be analyzed further to determine if they were malicious. \n",
    "\n",
    "Some preliminary work will be assumed, so please review the [first experiment](ApacheNGram.ipynb) for additional details.\n",
    "\n",
    "### Licence\n",
    "This work is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)\n",
    "![CC BY 4.0](https://licensebuttons.net/l/by/3.0/88x31.png)\n",
    "\n",
    "This license lets you distribute, remix, tweak, and build upon this work, even commercially, as long as you credit me, Robert Watkins, for the original creation. This is the most accommodating of licenses offered. Recommended for maximum dissemination and use of licensed materials.\n",
    "\n",
    "### Data Dictionary\n",
    "Using additional analysis not included in this notebook, I was able to come up with this data dictionary to describe the contents of the data file.\n",
    "\n",
    " | Column     | Description |\n",
    " |------------|:-----------:|\n",
    " | host       | The hostname or IP address making the request to the website |\n",
    " | logname    | not a populated field |\n",
    " | time       | integer version of unix timestamp |\n",
    " | method     | HTTP method for the call |\n",
    " | url        | path to the file being requested |\n",
    " | response   | HTTP response code for the call |\n",
    " | bytes      | Number of bytes returned for the request |\n",
    " | referer    | not a populated field |\n",
    " | useragent  | not a populated field |\n",
    "\n",
    "---\n",
    "\n",
    "### Prerequisites for this Python Notebook\n",
    " - python 3.x\n",
    " - python packages\n",
    "   - pip3 install pandas\n",
    "   - pip3 install scikit-learn\n",
    "   - pip3 install matplotlib\n",
    "   - pip3 install networkx\n",
    "   - pip3 install hdbscan\n",
    "   - pip3 install seaborn\n",
    " - jupyter notebook --debug > log.file 2>&1\n",
    " - mac users will need to install command-line tools for x-code\n",
    "    xcode-select --install\n",
    " \n",
    "### Sample data used for analysis\n",
    "- http://indeedeng.github.io/imhotep/files/nasa_19950801.tsv (saved in 'rawData/nasa_19950801.tsv')\n",
    "- http://indeedeng.github.io/imhotep/files/nasa_19950630.22-19950728.12.tsv.gz (Training Data)\n",
    "- http://indeedeng.github.io/imhotep/files/nasa_19950731.22-19950831.22.tsv.gz (Testing Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import math\n",
    "import hashlib \n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import itertools\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Next Experiment\n",
    "\n",
    "Ultimately, we are interested in determining if a pattern of usage for a new visitor looks 'typical' or 'a-typical'. In this case, we have existing usage that we will presume to be 'typical'. One common approach, which we'll be using, is the idea of 'clustering' our data. We need to group our usage of the system into usage where each usage of the system in the same group is close to each other. For a measure of 'closeness', we rely on being able to have a distance metric we can calculate.\n",
    "\n",
    "In this experiment, we'll calculate clusters using the [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance) also known as the 'edit distance' to compare a new path observed by a user to existing paths by other users. The Levenshtein Distance is called the 'edit distance' because it measures the number of edits it would take to change the first sequence to the second sequence. \n",
    "\n",
    "Rather than compare the full sequences of users' paths through the server to calculate the distance, we'll compare n-grams. It's not clear what choice of 'n' would be useful, so there will need to be some effort expended to choose a suitable 'n'.\n",
    "\n",
    "We'll correspondingly have a point at which we determine where the minimum Levenshtein Distance classifies the new sequence as 'typical' or 'a-typical'.\n",
    "\n",
    "This will result in having an optimized set of clusters and a corresponding configuration parameters to generate these clusters.\n",
    "\n",
    "### The Primary Challenge\n",
    "We don't have a good way to generate 'a-typcial' data in this case. So we're left at guessing a bit at what conditions will trigger an 'a-typical' categorization. Hopefully, using my current experience of how vulnerabiltiy-seeking tools and techniques work, I can come up with a sufficient variety of 'a-typical' calls to be able to test the model. \n",
    "\n",
    "Another challenge is that I'm not including any 'a-typical' data in my training. Any 'a-typical' behavior detected would be as a result of the user's path the system deviating sufficiently from normal navigation of the system and not from any other metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Levenschtein Distance for ordered lists of requested urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are fortunate to have a well-established distance formula available to caluate the 'distance' between two sequences of urls to determine how similar they are. One slight drawback is that there is no common library to calculate the levenschtein distance for a general orderd list of elements.\n",
    "\n",
    "The internet comes to the rescue to provide some tips on [rolling your own function](https://medium.com/@yash_agarwal2/soundex-and-levenshtein-distance-in-python-8b4b56542e9e) to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_levenshtein_distance(path1, path2):\n",
    "    \"\"\"\n",
    "    https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "    :param path1:\n",
    "    :param path2:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    matrix = [[0 for x in range(len(path2) + 1)] for x in range(len(path1) + 1)]\n",
    "\n",
    "    for x in range(len(path1) + 1):\n",
    "        matrix[x][0] = x\n",
    "    for y in range(len(path2) + 1):\n",
    "        matrix[0][y] = y\n",
    "\n",
    "    for x in range(1, len(path1) + 1):\n",
    "        for y in range(1, len(path2) + 1):\n",
    "            if path1[x - 1] == path2[y - 1]:\n",
    "                matrix[x][y] = min(\n",
    "                    matrix[x - 1][y] + 1,\n",
    "                    matrix[x - 1][y - 1],\n",
    "                    matrix[x][y - 1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix[x][y] = min(\n",
    "                    matrix[x - 1][y] + 1,\n",
    "                    matrix[x - 1][y - 1] + 1,\n",
    "                    matrix[x][y - 1] + 1\n",
    "                )\n",
    "\n",
    "    return matrix[len(path1)][len(path2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Methodology\n",
    "While the k-means clustering is seen as the de-facto standard for clustering, we won't be able to use it because it relies on the Euclidian space (using real numbers to identify points in the space). Our distance formula is strictly integer-based, so we need a [different approach](https://www.datascience.com/blog/k-means-alternatives).\n",
    "\n",
    "We'll be using [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) in the hopes that it provides a strong process to identify clusters from the noise and serves as a good method to distinquish 'a-typical' usage'.\n",
    "\n",
    "Additionally, we'll generate n-grams from a month's worth of data as our training data. Then we will generate n-grams from a month's worth of testing data and calcualte the minimum Levenshtein Distance for each n-gram in the hopes that we can identify the threshold where the distance is large enough to be classified as 'a-typical'.\n",
    "\n",
    "#### A few data formatting hoops\n",
    "The implementation of DBSCAN library I'm using only accepts numbers to perform it's calculations, so each url in our n-gram will need to be replaced with a unique numeric identifier. To achieve that, I'll create a url id map and functions to translate the n-grams back and forth from url to id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = 'rawData/nasa_19950801.tsv'\n",
    "training_data = pd.read_csv(training_data_path, sep='\\t', header=0)\n",
    "\n",
    "# get list of unique urls to get unique id.\n",
    "unique_urls_as_set = set(training_data['url'])\n",
    "unique_urls_as_list = list(unique_urls_as_set)\n",
    "unique_urls_as_array = np.array(unique_urls_as_list)\n",
    "unique_url_map = pd.DataFrame(unique_urls_as_array, columns={\"url\"})\n",
    "#print(unique_url_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to convert to/from id and url\n",
    "def get_id_from_url(url):\n",
    "    return unique_url_map.index[unique_url_map['url'] == url][0]\n",
    "\n",
    "def get_url_from_id(id):\n",
    "    return unique_url_map.loc[id,'url']\n",
    "\n",
    "\n",
    "# show how to convert from url to id and from id to url\n",
    "#print(\"Samples converting to/from urls and ids\")\n",
    "#print(\"URL with id 0 = \" , get_url_from_id(0))\n",
    "#print(\"id of url '/' = \", get_id_from_url('/'))\n",
    "\n",
    "#convert n-gram from urls to ids\n",
    "def n_gram_to_id (n_gram_of_urls):\n",
    "    n_gram_of_ids = list(n_gram_of_urls)\n",
    "    for i in range(0, len(n_gram_of_urls)):\n",
    "        n_gram_of_ids[i] = get_id_from_url(n_gram_of_urls[i])\n",
    "    return n_gram_of_ids\n",
    "\n",
    "#convert n-grams from ids to urls\n",
    "def n_gram_to_url (n_gram_of_ids):\n",
    "    n_gram_of_urls = list(n_gram_of_ids)\n",
    "    for i in range(0, len(n_gram_of_ids)):\n",
    "        n_gram_of_urls[i] = get_url_from_id(n_gram_of_ids[i])\n",
    "    return n_gram_of_urls\n",
    "\n",
    "#convert list of n-grams from urls to ids\n",
    "def n_gram_list_to_ids (n_gram_list_of_urls):\n",
    "    n_gram_list_of_ids = list(n_gram_list_of_urls)\n",
    "    for i in range(0, len(n_gram_list_of_urls)):\n",
    "        n_gram_list_of_ids[i] = n_gram_to_id(n_gram_list_of_urls[i])\n",
    "    return n_gram_list_of_ids\n",
    "\n",
    "#convert list of n-grams from ids to urls\n",
    "def n_gram_list_to_urls (n_gram_list_of_ids):\n",
    "    n_gram_list_of_urls = list(n_gram_list_of_ids)\n",
    "    for i in range(0, len(n_gram_list_of_ids)):\n",
    "        n_gram_list_of_urls[i] = n_gram_to_url(n_gram_list_of_ids[i])\n",
    "    return n_gram_list_of_urls\n",
    "\n",
    "#sample_n_gram = sample_n_gram_list[0]\n",
    "#print(\"sample ngram :\", sample_n_gram)\n",
    "#print(\"sample ngram to id :\", n_gram_to_id(sample_n_gram))\n",
    "\n",
    "#print(\"Starting ngram :\", sample_n_gram_list)\n",
    "\n",
    "#sample_n_gram_list_as_ids = n_gram_list_to_ids(sample_n_gram_list)\n",
    "#print(\"Converted to ngram of ids :\", sample_n_gram_list_as_ids)\n",
    "\n",
    "#print(\"Can it be converted back? \", sample_n_gram_list==n_gram_list_to_urls(sample_n_gram_list_as_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should unwind this a bit to make the purpose more clear\n",
    "#also, there should be padding added to the front/back to indicate beginning and ending of a path\n",
    "def find_ngrams(input_list, n):\n",
    "  return [list(x) for x in set(tuple(x) for x in list(zip(*[input_list[i:] for i in range(n)])))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify all the unique n-grams\n",
    "We've set aside the size of the n-grams so that we can experiment with what value this will need to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 n-grams :\n",
      " [['/', '/', '/', '/', '/'], ['/', '/', '/', '/', '/images/ksclogosmall.gif'], ['/', '/', '/', '/images/ksclogo-medium.gif', '/images/MOSAIC-logosmall.gif']]\n"
     ]
    }
   ],
   "source": [
    "#set value of 'n' for n-gram\n",
    "n = 5\n",
    "\n",
    "#Sort unique users by number of requests made to the web server\n",
    "visitor_addresses = training_data.host\n",
    "histogram_visitor_address = visitor_addresses.value_counts()\n",
    "\n",
    "# start with empty n-gram list\n",
    "ith_most_active_user_path_n_gram = list()\n",
    "\n",
    "for row in histogram_visitor_address.iteritems():\n",
    "    visitor = row[0]\n",
    "    #print(visitor)\n",
    "    #get just the entries from the second most active user\n",
    "    ith_most_active_user_logs = training_data.loc[training_data['host'] == visitor]\n",
    "    #get a list of just the URLs for that user\n",
    "    ith_most_active_user_path = ith_most_active_user_logs['url']\n",
    "\n",
    "    #create n-gram for graph\n",
    "    ith_most_active_user_path_n_gram.extend(list(find_ngrams(ith_most_active_user_path,n)))\n",
    "    \n",
    "    #remove duplicates\n",
    "    ith_most_active_user_path_n_gram.sort()\n",
    "    ith_most_active_user_path_n_gram = list(ith_most_active_user_path_n_gram for ith_most_active_user_path_n_gram,_ in itertools.groupby(ith_most_active_user_path_n_gram))\n",
    "\n",
    "print(\"First 3 n-grams :\\n\",ith_most_active_user_path_n_gram[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster n-grams with DBSCAN clustering and the Levenschtein Distance\n",
    "The two adjustment knobs we have for these experiments is the n in the n-grams and the minimum cluster size for DBSCAN. We'll modify these and see what the results look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate distance matrix, this could take quite a while\n",
    "sample_n_gram_list_as_ids = n_gram_list_to_ids(ith_most_active_user_path_n_gram)\n",
    "distance_matrix = pairwise_distances(sample_n_gram_list_as_ids, metric=get_levenshtein_distance)\n",
    "print(\"Distance Matrix: \\n\", distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the distance matrix calculate for our data, we'll be looping through the cluster sizes to try to find an optimal cluster size to minimize the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_leaf_size = 30\n",
    "my_eps = 3\n",
    "clustering = DBSCAN(algorithm='auto', eps=my_eps, leaf_size=my_leaf_size, metric='precomputed').fit_predict(distance_matrix)\n",
    "print(clustering.labels_.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
